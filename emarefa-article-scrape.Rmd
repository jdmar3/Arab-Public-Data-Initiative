---
title: "eMarefa scrape"
output: emarefa-scrape
---

This notebook contains code for scraping Search Engine Result Pages (SERPs) from https://search.emarefa.net. It uses rvest and RSelenium. Running the script below will install any packages necesary for the code contained in this document.

```{r}
# Check for required packages and install if not already installed
list.of.packages <- c("rvest", "RSelenium", "tidyverse")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Load required package list
lapply(list.of.packages, require, character.only = TRUE)
rm(new.packages,list.of.packages)
```

The code that follows loads the required packages, and then starts a Selenium server to navigate and scrape data from https://search.emarefa.net. Each SERP has button at the bottom of the page to load more results. Each page contains 15 results. The "Load more results" button loads them on the same SERP rather than loading an entirely new SERP. 

In order to capture the results of an entire search, the results have to be expanded fully before scraping and reading the html as results are not loaded simultaneously, but only when the button is manually clicked. The total number of results is given in the "load more" button. So, we scrape this number first and use it to compute the total number of clicks needed to expand all of the results for a given search URL. Then we use a repeat loop to click the button and load more results until it hits the end. Button clicks are spaced at a random interval between 2 and 10 seconds to minimize strain on server and also to appear as a more natural interaction so as not to be blocked by the target host. This perhaps should be raised to 20 seconds to give a wider range of random pauses, but that will increase the overall time it takes to expand all results. 

After all results are expanded by Selenium, we scrape the full SERP, and then parse it to extract columns corresponding to each field presented in the interface. We have started with the eMarefa record ID, title, and URL of the detail page for each record. In this case, we are pulling together only social science journals. This effectively gives us a list of titles, record numbers, and the URL for their detailed descriptions. 

```{r}
#Loading the rvest and RSelenium packages
library('rvest')
library('RSelenium')

### PART 1 ###
# Specifying the url for desired website to be scraped
baseurl <- "https://search.emarefa.net"
scrape_url <- "https://search.emarefa.net/social-sciences?query%5B0%5D%5Btext%5D=data+table&query%5B0%5D%5Bfield%5D=all&query%5B1%5D%5Boperator%5D=OR&query%5B1%5D%5Btext%5D=%D8%AC%D8%AF%D9%88%D9%84+%D8%A7%D9%84%D8%A8%D9%8A%D8%A7%D9%86%D8%A7%D8%AA&query%5B1%5D%5Bfield%5D=all&query%5B0%5D%5Btext%5D=survey&query%5B0%5D%5Bfield%5D=all&query%5B1%5D%5Boperator%5D=OR&query%5B1%5D%5Btext%5D=%D8%A7%D9%84%D8%AF%D8%B1%D8%A7%D8%B3%D8%A9+%D8%A7%D9%84%D8%A7%D8%B3%D8%AA%D9%82%D8%B5%D8%A7%D8%A6%D9%8A%D8%A9&query%5B1%5D%5Bfield%5D=all&query%5B0%5D%5Btext%5D=data&query%5B0%5D%5Bfield%5D=all&query%5B1%5D%5Boperator%5D=OR&query%5B1%5D%5Btext%5D=%D8%A7%D9%84%D8%A8%D9%8A%D8%A7%D9%86%D8%A7%D8%AA&query%5B1%5D%5Bfield%5D=all&query%5B0%5D%5Btext%5D=dataset&query%5B0%5D%5Bfield%5D=all&query%5B1%5D%5Boperator%5D=OR&query%5B1%5D%5Btext%5D=%D9%85%D8%AC%D9%85%D9%88%D8%B9%D8%A9+%D8%A7%D9%84%D8%A8%D9%8A%D8%A7%D9%86%D8%A7%D8%AA&query%5B1%5D%5Bfield%5D=all&query%5B0%5D%5Btext%5D=questionnaire&query%5B0%5D%5Bfield%5D=all&query%5B1%5D%5Boperator%5D=OR&query%5B1%5D%5Btext%5D=%D8%A5%D8%B3%D8%AA%D8%A8%D9%8A%D8%A7%D9%86&query%5B1%5D%5Bfield%5D=all&&has_full_text=0&query%5Blimiter%5D=&query%5Blimiter%5D=&lang=&page_no%5Blimiter%5D=equal&page_no%5Bnumber%5D=&type=&type%5B%5D=Journal+Articles&year%5B0%5D=&year%5B0%5D=2000&year%5B1%5D=&year%5B1%5D=2020"

# Download binaries, start driver, and get client object.
# This technique works in base R running on Linux. For other operation systems,
# see the notes in RSelenium vignettes below.
# https://cran.r-project.org/web/packages/RSelenium/vignettes/basics.html
# https://cran.r-project.org/web/packages/RSelenium/vignettes/saucelabs.html

rdriv <- rsDriver(port = 4444L, browser = "firefox")
# If port is already bound to a running server, there will be an error and you 
# must restart the session. Otherwise, find the PID of the server process and 
# kill it. See link below: 
# https://stackoverflow.com/questions/43991498/rselenium-server-signals-port-is-already-in-use
ffdriv <- rdriv$client

# Navigate to page.
ffdriv$navigate(scrape_url)

# Find total number of records
#total_records <- 
total_count_span <- ffdriv$findElement(using = "css", ".summery-totalCount")

total_count <- as.numeric(total_count_span$getElementText())

total_clicks <- floor(total_count/15)

repeat {
  total_clicks <- total_clicks-1
  cat("\r",total_clicks,"results expansions remaining. ")
  # Find the load button and assign, then send click event.
  load_btn <- ffdriv$findElement(using = "css selector", ".load-more-btn")
  load_btn$clickElement()
  # Wait for elements to load.
  seconds <- sample(2:10, 1, replace=T)
  cat("Waiting", seconds, "seconds until next expansion. ")
  Sys.sleep(seconds)
  if (total_clicks == 0){
    cat("\r","Results fully expanded.");
    break
  }
}

# Get start time
begin <- format(Sys.time(), "%Y%m%d%H%M%S")
# Set slug name
slug <- paste0(begin, "-emarefa-soc-sci-journal-articles")
# Create ISO-8601 date folder for cache
dir.create(paste0(slug))
# Create detail folder
dir.create(paste0(slug, "/detail/"))
# Get HTML data and parse
html_data <- ffdriv$getPageSource()[[1]]
#html_data <- read_html("./20200916015629-emarefa-soc-sci-journal-articles/20200916015629-emarefa-soc-sci-journal-articles-LISTING.html")
# Write html_data to file
write(html_data, file=paste0(slug, "/", slug, "-LISTING.html"))
# Extract record ID
record_id <- html_data %>%
  read_html() %>%
  html_nodes(".content-row") %>%
  html_attr("data-key")
# Extract title
title <- html_data %>% 
  read_html() %>% 
  html_nodes(".title-link h2") %>% 
  html_text()
# Construct canonical detail url
canonical <- paste0(baseurl, "/detail/", record_id)
# Build data frame
record_list <- data.frame(record_id, canonical, title)
# Clean up unused values
#rm(title, record_id, detail_url_trunc, detail_url, html_data)
### PART 2 ###
### Loop over list of links to detail pages
d <- vector("list", length(canonical))


for (i in seq_along(canonical)) {
  # Check for existing file
  filepath <- paste0("./", slug, "/detail/", record_id[i], ".html")
  # Write local copy of scraped HTML
  if (!file.exists(filepath)) {
    # Read in html from detail page listed in records_list
    html.loop <- read_html(canonical[i])
    write_html(html.loop, file = filepath)
    seconds <- sample(1:5, 1, replace=T)
    cat("\r", record_id[i], "downloaded. Getting next record in", seconds, "seconds. ")
    # Sleep for random number of seconds
    Sys.sleep(seconds)
  } else {
    cat("\r", record_id[i], "already downloaded. Skipping.")
  }
}
# 
# for (i in thing) {
# # Extract metadata fieldnames
#   varnames.loop <- html_nodes(html.loop, ".inner-container > h3") %>% html_text()
# # Clean up fieldnames for insertion in records_lsit dataframe
# # Remove "Topics" because it doesn't parse the same as the other fields and is
# # unnecessary for our purposes
#   varnames.loop <- varnames.loop[!grepl("Topics", varnames.loop, ignore.case = TRUE)]
# # Remove punctuation and special characters from variable names
#   varnames.loop <- gsub("[[:punct:]]", "", varnames.loop)
# # force lowercase and replace all spaces with underscores
#   varnames.loop <- tolower(gsub(" ", "_", varnames.loop))
# # Build an empty dataframe with the varnames as column names
#   df.loop <- data.frame(matrix(ncol = length(varnames.loop), nrow = 0))
#   names(df.loop) <- varnames.loop
# 
#   values.loop <- html_nodes(html.loop, ".inner-container > p") %>% html_text()
# 
#   df.loop[1, ] <- as.list(values.loop)
# 
#   df.loop$abstract_en <-
#     html_node(html.loop, "#EN_ABSTRACT > p") %>% html_text()
# 
#   df.loop$abstract_ar <-
#     html_node(html.loop, "#AR_ABSTRACT > p") %>% html_text()
# 
#   df.loop$abstract_fr <-
#     html_node(html.loop, "#FR_ABSTRACT > p") %>% html_text()
# # Write to dataframe
#   record_list <- merge(df.loop, record_list, all.x = TRUE, all.y = TRUE)
# ### End loop
# }
# 
# # Write data frame out to CSV with datetimestamp appended
# write.csv2(record_list, file = paste0(slug, "/", slug, "-DETAILS.csv"))
# # Write data frame out to RDATA with datetimestamp appended
# save(record_list, file = paste0(paste0(slug, "/", slug, "-DETAILS.Rdata")))
# # Record end time
# # end <- format(Sys.time(), "%Y%m%d%H%M%S")
# # Close ports, stop RSelenium server, garbage collection 
# #rdriv$close()
# #ffdriv$server$stop()
# rm(rdriv)
# gc()
```
