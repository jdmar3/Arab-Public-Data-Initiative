---
title: "e-Marefa scrape"
output: emarefa-scrape
---

This notebook contains code for scraping Search Engine Result Pages (SERPs) from https://search.emarefa.net. It uses rvest and RSelenium. Running the script below will install any packages necesary for the code contained in this document.

```{r}
# Check for required packages and install if not already installed
list.of.packages <- c("rvest", "RSelenium", "tidyverse")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Load required package list
lapply(list.of.packages, require, character.only = TRUE)
rm(new.packages,list.of.packages)
```

The code that follows loads the required packages, and then starts a Selenium server to navigate and scrape data from https://search.emarefa.net. Each SERP has button at the bottom of the page to load more results. Each page contains 15 results. The "Load more results" button loads them on the same SERP rather than loading an entirely new SERP. 

In order to capture the results of an entire search, the results have to be expanded fully before scraping and reading the html as results are not loaded simultaneously, but only when the button is manually clicked. The total number of results is given in the "load more" button. So, we scrape this number first and use it to compute the total number of clicks needed to expand all of the results for a given search URL. Then we use a repeat loop to click the button and load more results until it hits the end. Button clicks are spaced at a random interval between 2 and 10 seconds to minimize strain on server and also to appear as a more natural interaction so as not to be blocked by the target host. This perhaps should be raised to 20 seconds to give a wider range of random pauses, but that will increase the overall time it takes to expand all results. 

After all results are expanded by Selenium, we scrape the full SERP, and then parse it to extract columns corresponding to each field presented in the interface. We have started with the eMarefa record ID, title, and URL of the detail page for each record. In this case, we are pulling together only social science journals. This effectively gives us a list of titles, record numbers, and the URL for their detailed descriptions. 

```{r}
#Loading the rvest package
library('rvest')
library('RSelenium')

#Specifying the url for desired website to be scraped
baseurl <- "https://search.emarefa.net"
scrape_url <- "https://search.emarefa.net/social-sciences?type%5B%5D=Journals&year%5B0%5D=2000&year%5B1%5D=2020&order=date_desc&has_full_text=0&page_no%5Blimiter%5D=equal"

# Download binaries, start driver, and get client object.
# This technique works in base R running on Linux. For other operation systems,
# see the notes in RSelenium vignettes below.
# https://cran.r-project.org/web/packages/RSelenium/vignettes/basics.html
# https://cran.r-project.org/web/packages/RSelenium/vignettes/saucelabs.html

rdriv <- rsDriver(port = 4444L, browser = "firefox")
# If port is already bound to a running server, there will be an error and you 
# must restart the session. Otherwise, find the PID of the server process and 
# kill it. See link below: 
# https://stackoverflow.com/questions/43991498/rselenium-server-signals-port-is-already-in-use
ffdriv <- rdriv$client

# Navigate to page.
ffdriv$navigate(scrape_url)

# Find total number of records
#total_records <- 
total_count_span <- ffdriv$findElement(using = "css", ".summery-totalCount")

total_count <- as.numeric(total_count_span$getElementText())

total_clicks <- floor(total_count/15)

repeat {
  total_clicks <- total_clicks-1
  cat("\r",total_clicks,"results expansions remaining. ")
  # Find the load button and assign, then send click event.
  load_btn <- ffdriv$findElement(using = "css selector", ".load-more-btn")
  load_btn$clickElement()
  # Wait for elements to load.
  seconds <- sample(2:10, 1, replace=T)
  cat("Waiting", seconds, "seconds until next expansion. ")
  Sys.sleep(seconds)
  if (total_clicks == 0){
    cat("Results fully expanded.")
  }
}

# Get HTML data and parse
html_data <- ffdriv$getPageSource()[[1]]
# Extract record ID
record_id <- html_data %>%
  read_html() %>%
  html_nodes(".content-row") %>%
  html_attr("data-key")
# Extract title
title <- html_data %>% 
  read_html() %>% 
  html_nodes(".title-link h2") %>% 
  html_text()
# Extract detail URL
detail_url_trunc <- html_data %>% 
  read_html() %>% 
  html_nodes(".title-link") %>% 
  html_attr("href")
# Append base URL to detail URL
detail_url <- paste0(baseurl,detail_url_trunc)
# Build data frame
record_list <- data.frame(title, record_id, detail_url)
# Write data frame out to CSV with datetimestamp appended
write.csv2(record_list, file = paste0(format(Sys.time(), "%Y%m%d%H%M%S"), "-emarefa-soc-sci-journals.csv"))
# Write data frame out to RDATA with datetimestamp appended
save(record_list, file = paste0(format(Sys.time(), "%Y%m%d%H%M%S"), "-emarefa-soc-sci-journals.Rdata"))

# Close ports, stop RSelenium server, garbage collection 
rdriv$close()
ffdriv$server$stop()
rm(rdriv)
gc()
```


